/**
 * !!! NB !!!
 * This approach is borrowed and extended from the 3rd-party Eigen wrapper/implementation in PyTorch.
 * It is NOT smart, NOT extensible, NOT flexiable, NOT adaptive,
 * but straightforward and good for early iterations of development.
 * Smarter ones are expected to be implemented in futuer iterations.
 */
#ifndef FLASHINFER_GPU_DEFINES_CUDA_HIP_H_
#define FLASHINFER_GPU_DEFINES_CUDA_HIP_H_


// To work around some unexpected HIPify behavior
#if defined(__HIPCC__) || (defined(__clang__) && defined(__HIP__)) || defined(__HIPCC_RTC__)

#define gpuStream_t hipStream_t
#define gpuDeviceProp_t hipDeviceProp_t
#define gpuError_t hipError_t
#define gpuSuccess hipSuccess
#define gpuErrorNotReady hipErrorNotReady
#define gpuGetDeviceCount hipGetDeviceCount
#define gpuGetLastError hipGetLastError
#define gpuPeekAtLastError hipPeekAtLastError
#define gpuGetErrorName hipGetErrorName
#define gpuGetErrorString hipGetErrorString
#define gpuGetDeviceProperties hipGetDeviceProperties
#define gpuStreamDefault hipStreamDefault
#define gpuGetDevice hipGetDevice
#define gpuSetDevice hipSetDevice
#define gpuMalloc hipMalloc
#define gpuFree hipFree
#define gpuMemsetAsync hipMemsetAsync
#define gpuMemcpyAsync hipMemcpyAsync
#define gpuMemcpyDeviceToDevice hipMemcpyDeviceToDevice
#define gpuMemcpyDeviceToHost hipMemcpyDeviceToHost
#define gpuMemcpyHostToDevice hipMemcpyHostToDevice
#define gpuStreamQuery hipStreamQuery
#define gpuSharedMemConfig hipSharedMemConfig
#define gpuDeviceSetSharedMemConfig hipDeviceSetSharedMemConfig
#define gpuStreamSynchronize hipStreamSynchronize
#define gpuDeviceSynchronize hipDeviceSynchronize
#define gpuMemcpy hipMemcpy
#define gpuDeviceGetAttribute hipDeviceGetAttribute
#define gpuFuncSetAttribute hipFuncSetAttribute
#define gpuLaunchKernel hipLaunchKernel
#define gpuFreeHost hipHostFree
#define gpuMallocHost hipHostMalloc
#define gpuOccupancyMaxActiveBlocksPerMultiprocessor hipOccupancyMaxActiveBlocksPerMultiprocessor
#define gpuDevAttrMaxSharedMemoryPerMultiprocessor hipDeviceAttributeMaxSharedMemoryPerMultiprocessor
#define gpuFuncAttributeMaxDynamicSharedMemorySize hipFuncAttributeMaxDynamicSharedMemorySize
#define gpuDevAttrMultiProcessorCount hipDeviceAttributeMultiprocessorCount
#define gpuDevAttrComputeCapabilityMajor hipDeviceAttributeComputeCapabilityMajor
#define gpuDevAttrComputeCapabilityMinor hipDeviceAttributeComputeCapabilityMinor

// BLAS/BLATLt
#define gpublasStatus_t hipblasStatus_t
#define gpublasLtMatmulDescOpaque_t hipblasLtMatmulDescOpaque_t
#define gpublasLtMatrixLayoutOpaque_t hipblasLtMatrixLayoutOpaque_t
#define gpublasComputeType_t hipblasComputeType_t
#define gpuDataType_t hipDataType
#define gpublasLtMatmulDesc_t hipblasLtMatmulDesc_t
#define gpublasLtMatmulDescAttributes_t hipblasLtMatmulDescAttributes_t
#define gpublasLtMatrixLayout_t hipblasLtMatrixLayout_t
#define gpublasLtMatrixLayoutAttribute_t hipblasLtMatrixLayoutAttribute_t
#define gpublasLtMatmulPreferenceOpaque_t hipblasLtMatmulPreferenceOpaque_t
#define gpublasLtMatmulPreference_t hipblasLtMatmulPreference_t
#define gpublasLtMatmulPreferenceAttributes_t hipblasLtMatmulPreferenceAttributes_t
#define gpublasLtHandle_t hipblasLtHandle_t
#define gpublasLtMatmulHeuristicResult_t hipblasLtMatmulHeuristicResult_t
#define gpublasLtMatmulDescCreate hipblasLtMatmulDescCreate
#define gpublasLtMatmulDescSetAttribute hipblasLtMatmulDescSetAttribute
#define gpublasLtMatrixLayoutDestroy hipblasLtMatrixLayoutDestroy
#define gpublasLtMatrixLayoutCreate hipblasLtMatrixLayoutCreate
#define gpublasLtMatrixLayoutSetAttribute hipblasLtMatrixLayoutSetAttribute
#define gpublasLtMatmulPreferenceDestroy hipblasLtMatmulPreferenceDestroy
#define gpublasLtMatmulPreferenceCreate hipblasLtMatmulPreferenceCreate
#define gpublasLtMatmulPreferenceSetAttribute hipblasLtMatmulPreferenceSetAttribute
#define gpublasLtMatmulAlgoGetHeuristic hipblasLtMatmulAlgoGetHeuristic
#define gpublasLtMatmulDescDestroy hipblasLtMatmulDescDestroy
#define gpublasLtMatmul hipblasLtMatmul
#define GPUBLAS_STATUS_SUCCESS HIPBLAS_STATUS_SUCCESS
#define GPUBLAS_STATUS_NOT_SUPPORTED HIPBLAS_STATUS_NOT_SUPPORTED
#define GPU_R_8F_E4M3 HIPBLAS_R_8F_E4M3
#define GPU_R_8F_E5M2 HIPBLAS_R_8F_E5M2
#define GPU_R_16BF HIP_R_16BF
#define GPU_R_16F HIP_R_16F
#define GPUBLAS_COMPUTE_32F HIPBLAS_COMPUTE_32F
#define GPUBLASLT_MATMUL_DESC_TRANSA HIPBLASLT_MATMUL_DESC_TRANSA
#define GPUBLASLT_MATMUL_DESC_TRANSB HIPBLASLT_MATMUL_DESC_TRANSB
#define GPUBLAS_OP_T HIPBLAS_OP_T
#define GPUBLASLT_MATMUL_DESC_FAST_ACCUM HIPBLASLT_MATMUL_DESC_FAST_ACCUM
#define GPUBLASLT_MATMUL_DESC_A_SCALE_POINTER HIPBLASLT_MATMUL_DESC_A_SCALE_POINTER
#define GPUBLASLT_MATMUL_DESC_B_SCALE_POINTER HIPBLASLT_MATMUL_DESC_B_SCALE_POINTER
#define GPUBLASLT_MATRIX_LAYOUT_BATCH_COUNT HIPBLASLT_MATRIX_LAYOUT_BATCH_COUNT
#define GPUBLASLT_MATRIX_LAYOUT_STRIDED_BATCH_OFFSET HIPBLASLT_MATRIX_LAYOUT_STRIDED_BATCH_OFFSET
#define GPUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES HIPBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES

// float8 Precision Device types
#define __gpu_fp8_e4m3 __hip_fp8_e4m3_fnuz
#define __gpu_fp8_e5m2 __hip_fp8_e5m2_fnuz
#define __gpu_fp8x2_e4m3 __hip_fp8x2_e4m3_fnuz
#define __gpu_fp8x2_e5m2 __hip_fp8x2_e5m2_fnuz
#define __gpu_fp8x2_storage_t __hip_fp8x2_storage_t
#define __gpu_fp8x4_storage_t __hip_fp8x4_storage_t
#define __gpu_fp8x4_e4m3 __hip_fp8x4_e4m3_fnuz
#define __gpu_fp8x4_e5m2 __hip_fp8x4_e5m2_fnuz
// Bfloat16 Precision Device types
// https://github.com/ROCm/ROCm/issues/2534
// #define gpu_bfloat16 hip_bfloat16
#define gpu_bfloat16 __hip_bfloat16
#define __gpu_bfloat16 __hip_bfloat16
#define gpu_bfloat162 __hip_bfloat162
#define __gpu_bfloat162 __hip_bfloat162


// #elif defined(__CUDACC__) || defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__)) || defined(__CUDACC_RTC__)
#else


#define gpuStream_t cudaStream_t
#define gpuDeviceProp_t cudaDeviceProp
#define gpuError_t cudaError_t
#define gpuSuccess cudaSuccess
#define gpuErrorNotReady cudaErrorNotReady
#define gpuGetDeviceCount cudaGetDeviceCount
#define gpuGetLastError cudaGetLastError
#define gpuPeekAtLastError cudaPeekAtLastError
#define gpuGetErrorName cudaGetErrorName
#define gpuGetErrorString cudaGetErrorString
#define gpuGetDeviceProperties cudaGetDeviceProperties
#define gpuStreamDefault cudaStreamDefault
#define gpuGetDevice cudaGetDevice
#define gpuSetDevice cudaSetDevice
#define gpuMalloc cudaMalloc
#define gpuFree cudaFree
#define gpuMemsetAsync cudaMemsetAsync
#define gpuMemcpyAsync cudaMemcpyAsync
#define gpuMemcpyDeviceToDevice cudaMemcpyDeviceToDevice
#define gpuMemcpyDeviceToHost cudaMemcpyDeviceToHost
#define gpuMemcpyHostToDevice cudaMemcpyHostToDevice
#define gpuStreamQuery cudaStreamQuery
#define gpuSharedMemConfig cudaSharedMemConfig
#define gpuDeviceSetSharedMemConfig cudaDeviceSetSharedMemConfig
#define gpuStreamSynchronize cudaStreamSynchronize
#define gpuDeviceSynchronize cudaDeviceSynchronize
#define gpuMemcpy cudaMemcpy
#define gpuDeviceGetAttribute cudaDeviceGetAttribute
#define gpuFuncSetAttribute cudaFuncSetAttribute
#define gpuLaunchKernel cudaLaunchKernel
#define gpuFreeHost cudaFreeHost
#define gpuMallocHost cudaMallocHost
#define gpuOccupancyMaxActiveBlocksPerMultiprocessor cudaOccupancyMaxActiveBlocksPerMultiprocessor
#define gpuDevAttrMaxSharedMemoryPerMultiprocessor cudaDevAttrMaxSharedMemoryPerMultiprocessor
#define gpuFuncAttributeMaxDynamicSharedMemorySize cudaFuncAttributeMaxDynamicSharedMemorySize
#define gpuDevAttrMultiProcessorCount cudaDevAttrMultiProcessorCount
#define gpuDevAttrComputeCapabilityMajor cudaDevAttrComputeCapabilityMajor
#define gpuDevAttrComputeCapabilityMinor cudaDevAttrComputeCapabilityMinor

// BLAS/BLASLt
#define gpublasStatus_t cublasStatus_t
#define gpublasLtMatmulDescOpaque_t cublasLtMatmulDescOpaque_t
#define gpublasLtMatrixLayoutOpaque_t cublasLtMatrixLayoutOpaque_t
#define gpublasComputeType_t cublasComputeType_t
#define gpuDataType_t cudaDataType_t
#define gpublasLtMatmulDesc_t cublasLtMatmulDesc_t
#define gpublasLtMatmulDescAttributes_t cublasLtMatmulDescAttributes_t
#define gpublasLtMatrixLayout_t cublasLtMatrixLayout_t
#define gpublasLtMatrixLayoutAttribute_t cublasLtMatrixLayoutAttribute_t
#define gpublasLtMatmulPreferenceOpaque_t cublasLtMatmulPreferenceOpaque_t
#define gpublasLtMatmulPreference_t cublasLtMatmulPreference_t
#define gpublasLtMatmulPreferenceAttributes_t cublasLtMatmulPreferenceAttributes_t
#define gpublasLtHandle_t cublasLtHandle_t
#define gpublasLtMatmulHeuristicResult_t cublasLtMatmulHeuristicResult_t
#define gpublasLtMatmulDescCreate hipblasLtMatmulDescCreate
#define gpublasLtMatmulDescSetAttribute cublasLtMatmulDescSetAttribute
#define gpublasLtMatrixLayoutDestroy cublasLtMatrixLayoutDestroy
#define gpublasLtMatrixLayoutCreate cublasLtMatrixLayoutCreate
#define gpublasLtMatrixLayoutSetAttribute cublasLtMatrixLayoutSetAttribute
#define gpublasLtMatmulPreferenceDestroy cublasLtMatmulPreferenceDestroy
#define gpublasLtMatmulPreferenceCreate cublasLtMatmulPreferenceCreate
#define gpublasLtMatmulPreferenceSetAttribute cublasLtMatmulPreferenceSetAttribute
#define gpublasLtMatmulAlgoGetHeuristic cublasLtMatmulAlgoGetHeuristic
#define gpublasLtMatmulDescDestroy cublasLtMatmulDescDestroy
#define gpublasLtMatmul cublasLtMatmul
#define GPUBLAS_STATUS_SUCCESS CUBLAS_STATUS_SUCCESS
#define GPUBLAS_STATUS_NOT_SUPPORTED CUBLAS_STATUS_NOT_SUPPORTED
#define GPU_R_8F_E4M3 CUDA_R_8F_E4M3
#define GPU_R_8F_E5M2 CUDA_R_8F_E5M2
#define GPU_R_16BF CUDA_R_16BF
#define GPU_R_16F CUDA_R_16F
#define GPUBLAS_COMPUTE_32F CUBLAS_COMPUTE_32F
#define GPUBLASLT_MATMUL_DESC_TRANSA CUBLASLT_MATMUL_DESC_TRANSA
#define GPUBLASLT_MATMUL_DESC_TRANSB CUBLASLT_MATMUL_DESC_TRANSB
#define GPUBLAS_OP_T CUBLAS_OP_T
#define GPUBLASLT_MATMUL_DESC_FAST_ACCUM CUBLASLT_MATMUL_DESC_FAST_ACCUM
#define GPUBLASLT_MATMUL_DESC_A_SCALE_POINTER CUBLASLT_MATMUL_DESC_A_SCALE_POINTER
#define GPUBLASLT_MATMUL_DESC_B_SCALE_POINTER CUBLASLT_MATMUL_DESC_B_SCALE_POINTER
#define GPUBLASLT_MATRIX_LAYOUT_BATCH_COUNT CUBLASLT_MATRIX_LAYOUT_BATCH_COUNT
#define GPUBLASLT_MATRIX_LAYOUT_STRIDED_BATCH_OFFSET CUBLASLT_MATRIX_LAYOUT_STRIDED_BATCH_OFFSET
#define GPUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES CUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES

// float8 Precision Device types
#define __gpu_fp8_e4m3 __nv_fp8_e4m3
#define __gpu_fp8_e5m2 __nv_fp8_e5m2
#define __gpu_fp8x2_e4m3 __nv_fp8x2_e4m3
#define __gpu_fp8x2_e5m2 __nv_fp8x2_e5m2
#define __gpu_fp8x2_storage_t __nv_fp8x2_storage_t
#define __gpu_fp8x4_storage_t __nv_fp8x4_storage_t
#define __gpu_fp8x4_e4m3 __nv_fp8x4_e4m3
#define __gpu_fp8x4_e5m2 __nv_fp8x4_e5m2
// Bfloat16 Precision Device types
#define gpu_bfloat16 nv_bfloat16
#define __gpu_bfloat16 __nv_bfloat16
#define gpu_bfloat162 nv_bfloat162
#define __gpu_bfloat162 __nv_bfloat162


#endif  // __CUDACC__ or __HIPCC__


// `gpu_assert` can be overridden.
#ifndef gpu_assert

#ifdef __HIP_DEVICE_COMPILE__
// HIPCC does not support the use of assert on the GPU side.
#define gpu_assert(COND)
#else
#define gpu_assert(COND) assert(COND)
#endif

#endif  // gpu_assert


#endif  // FLASHINFER_GPU_DEFINES_CUDA_HIP_H_
